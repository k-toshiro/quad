import functools
import os
import sys
import json
import warnings
import pathlib

warnings.filterwarnings('ignore', '.*box bound precision lowered.*')
warnings.filterwarnings('ignore', '.*using stateful random seeds*')

directory = pathlib.Path(__file__).resolve().parent
sys.path.append(str(directory.parent.parent))
# __package__ = directory.name
# print(sys.path)

import embodied


def main(argv):
  if argv[0] == "pydreamer":
    from embodied.agents.dreamervpy.agent import Agent
  elif argv[0] == "dreamerv2":
    from embodied.agents.dreamerv2.agent import Agent
  elif argv[0] == "dreamerv3":
    from embodied.agents.dreamerv3wip.agent import Agent
  else:
    assert False, f"{argv[0]} should be the name of the agent"

  config, parsed = build_config(Agent.configs, argv[1:])
  logdir = embodied.Path(config.logdir)

  step = embodied.Counter()
  logger = build_logger(config, parsed, step)

  # if config.expl_from_seeds:
  #   frac = config.seed / config.expl_from_seeds
  #   config = config.update({'actent.target': 0.5 * frac + 0.1})

  # if config.run != 'train_async_gpu':
  #   embodied.Worker.add_initializer(lambda: os.environ.update(
  #       'CUDA_VISIBLE_DEVICES'='-1'))

  env = embodied.envs.load_env(config.task, mode='train', **config.env)
  agent = Agent(env.obs_space, env.act_space, step, config)
  args = embodied.Config(logdir=config.logdir, **config.train)  # train args

  cleanup = []
  try:
    if config.run == 'train':
      replay = build_replay(config, logdir / 'episodes')
      embodied.run.train(agent, env, replay, logger, args)

    elif config.run == 'train_async_gpu':
      replay = build_replay(config, logdir / 'episodes')
      embodied.run.train_async_gpu(agent, env, replay, logger, args)

    elif config.run == 'train_eval':
      eval_env = embodied.envs.load_env(config.task, mode='eval', **config.env)
      replay = build_replay(config, logdir / 'episodes')
      eval_replay = build_replay(
          config, logdir / 'eval_episodes',
          is_eval=True,
          length=config.tbtt or config.replay.length,
          capacity=config.replay.capacity // 10)
      embodied.run.train_eval(
          agent, env, eval_env, replay, eval_replay, logger, args)
      cleanup.append(eval_env)

    elif config.run == 'train_fixed_eval' and config.eval_dir:
      assert not config.train.eval_fill
      replay = build_replay(config, logdir / 'episodes')
      eval_replay = build_replay(
          config, config.eval_dir,
          is_eval=True,
          length=config.tbtt or config.replay.length,
          capacity=config.replay.capacity // 10)
      embodied.run.train_fixed_eval(
          agent, env, replay, eval_replay, logger, args)

    elif config.run == 'train_fixed_eval':
      assert config.train.eval_fill
      replay = build_replay(config, logdir / 'episodes')
      eval_replay = build_replay(
          config, logdir / 'eval_episodes',
          is_eval=True,
          length=config.tbtt or config.replay.length,
          capacity=config.replay.capacity // 10)
      embodied.run.train_fixed_eval(
          agent, env, replay, eval_replay, logger, args)

    elif config.run == 'learning':
      env.close()
      replay = build_replay(config, logdir / 'episodes')
      if config.eval_dir:
        # Separate eval_dir
        eval_replay = build_replay(
            config, config.eval_dir,
            is_eval=True,
            length=config.tbtt or config.replay.length,
            capacity=config.replay.capacity // 10)
      elif int(parsed.actors) >= 2:
        # Holdout data generated by actor1
        eval_replay = build_replay(
            config, logdir / 'eval_episodes',
            is_eval=True,
            length=config.tbtt or config.replay.length,
            capacity=config.replay.capacity // 10,
            rescan=300)
      else:
        # No holdout data, eval on train
        eval_replay = replay
      replay = embodied.replay.ReplayServer(replay, port=parsed.learner_addr.split(':')[-1])
      embodied.run.learning(agent, replay, eval_replay, logger, args)

    elif config.run == 'acting':
      if parsed.actor == 1:
        # Use actor1 to generate holdout eval data (not actor0 in case we are testing locally with actor=0)
        actor_replay = build_replay(
            config, logdir / 'eval_episodes',
            is_eval=True,
            length=config.tbtt or config.replay.length,
            capacity=args.train_fill)
      else:
        actor_replay = embodied.replay.ReplayClient(parsed.learner_addr)
      embodied.run.acting(agent, env, actor_replay, logger, parsed.actordir, args)

    else:
      raise NotImplementedError(config.run)

  finally:
    for obj in cleanup:
      obj.close()


def build_config(all_configs, argv):
  # Parse --configs arg
  parsed, other = embodied.Flags(
      configs=['defaults'],
      actor=0,
      actors=0,
      actordir='',
      learner_addr='localhost:2222',
  ).parse_known(argv)

  # Parse TF_CONFIG for distributed run
  if 'TF_CONFIG' in os.environ:
    tf_config = json.loads(os.environ['TF_CONFIG'])
    print('TF_CONFIG:', tf_config)
    learner_addr = tf_config['cluster']['chief'][0]
    parsed = parsed.update(learner_addr=learner_addr)
  else:
    tf_config = None

  if parsed.actor == -1:
    assert tf_config is not None, 'Need TF_CONFIG to tell which actor this is'
    ix_actor = tf_config['task']['index']
    parsed = parsed.update(actor=ix_actor)

  # Take a union of default and --configs configs
  config = embodied.Config(all_configs['defaults'])
  for name in parsed.configs:
    config = config.update(all_configs[name])

  # Override from command line flags
  config = embodied.Flags(config).parse(other)

  # Tweak values
  config = config.update(logdir=str(embodied.Path(str(config.logdir))))
  if 'env.seed' in config:
    config = config.update({'env.seed': hash((config.seed, parsed.actor))})
  if config.run == 'acting':
    parsed = parsed.update(
        actordir=str(embodied.Path(config.logdir) / f'actor{parsed.actor}'))

  return config, parsed


def build_logger(config, parsed, step):

  logdir = embodied.Path(str(config.logdir))
  multiplier = config.env.repeat
  if config.run == 'acting':
    logdir = embodied.Path(parsed.actordir)
    multiplier *= parsed.actors
  if config.run == 'learning':
    multiplier = 1

  log_outputs = [
      embodied.logger.TerminalOutput(),
      embodied.logger.JSONLOutput(logdir),
  ]
  if not os.environ.get('NO_TENSORBOARD'):
    import tensorflow
    log_outputs.append(
        embodied.logger.TensorBoardOutput(logdir)
    )
  if os.environ.get('MLFLOW_TRACKING_URI'):
    if config.run == 'acting':
      if parsed.actor < 3:  # Only log from 3 actors in distributed case
        log_outputs.append(
            embodied.logger.MlflowOutput(params=config.flat, prefix=f'actor{parsed.actor}/')
        )
    else:
      log_outputs.append(
          embodied.logger.MlflowOutput(params=config.flat)
      )

  logger = embodied.Logger(step, log_outputs, multiplier=multiplier)
  return logger


def build_replay(config, outdir, is_eval=False, **kwargs) -> embodied.Replay:
  indirs = [outdir]
  # if config.read_from_seeds:
  #   assert config.replay.rescan or config.prio_replay.rescan
  #   for seed in set(range(config.read_from_seeds)) - {config.seed}:
  #     indirs.append(logdir.parent / str(seed) / 'episodes')
  # if config.warm_start_dir:
  #   indirs.append(config.warm_start_dir)

  # if config.clone_success:
  #   def preproc(ep):
  #     success = (np.cumsum(ep['reward'][::-1])[::-1] > 1e-5)
  #     ep['bc'] = 0.01 * success.astype(np.float32)
  #     return ep
  #   Replay = functools.partial(Replay, preproc=preproc)

  if config.replay_type == 'ram':
    replay = embodied.replay.RamReplay(outdir, indirs, **dict(config.replay, **kwargs))
  elif config.replay_type == 'disk':
    replay = embodied.replay.DiskReplay(outdir, indirs, **dict(config.replay, **kwargs))
  elif config.replay_type == 'prio':
    if is_eval:
      # Don't use prioritized for eval
      replay = embodied.replay.RamReplay(outdir, indirs, **dict(config.replay, **kwargs))
    else:
      replay = embodied.replay.PrioReplay(outdir, indirs, **dict(config.prio_replay, **kwargs))
  else:
    assert False, config.replay_type

  # if config.demos_prob > 0.0:
  #   demos = Replay(config.demos_dir)
  #   assert demos.stats['total_steps'] > 0
  #   replay = embodied.replay.MultiReplay(
  #       writers=[replay], readers=[replay, demos],
  #       read_probs=[1.0, config.demos_prob])

  if config.get('tbtt'):
    replay = embodied.replay.Consecutive(replay, config.tbtt)

  return replay


if __name__ == '__main__':
  main(sys.argv[1:])
